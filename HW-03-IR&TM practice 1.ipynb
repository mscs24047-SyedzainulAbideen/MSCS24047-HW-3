{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7210450-8376-47a4-8eee-a6b349e02846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\syed zain ul aideen\\appdata\\roaming\\python\\python313\\site-packages (0.2.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\syed zain ul aideen\\appdata\\roaming\\python\\python313\\site-packages (4.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\syed zain ul aideen\\appdata\\roaming\\python\\python313\\site-packages (1.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\syed zain ul aideen\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn nltk rank-bm25 gensim matplotlib tqdm faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49dc080f-8dc4-4b96-b844-6e86842b859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: rank_bm25 in c:\\users\\syed zain ul aideen\\appdata\\roaming\\python\\python313\\site-packages (0.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Only run if packages not installed\n",
    "!pip install numpy pandas scikit-learn nltk rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd83ca6-d4bd-40a4-aefe-e097924af2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no', 'will', 'same', \"they'd\", 'between', 'into', 'only', 'again', 'further', 'had', 'theirs', 'doing', 'other', 'than', 'their', \"hasn't\", \"won't\", 'your', 'such', \"weren't\", 'haven', 'herself', \"i'd\", \"you'd\", \"wouldn't\", 'below', \"we'd\", \"it'll\", 'did', 'nor', 'there', 'doesn', 'whom', 'hers', 'itself', \"he'll\", 'yourselves', 'they', \"you've\", \"we've\", 'when', 'on', 'been', 'was', 'after', 'ours', 'isn', 'ourselves', 'both', 'at', 'weren', 'which', 'why', \"mustn't\", \"it's\", \"you're\", \"isn't\", 'own', 'ain', \"shan't\", 'over', \"mightn't\", 'an', \"i've\", 'y', \"she's\", 'our', 'o', 'if', 'what', 'd', \"aren't\", 'then', 'the', \"wasn't\", 'while', 'against', \"she'll\", 'more', 'during', 'themselves', 'didn', 'how', 'for', 'under', 'won', \"i'll\", 'so', 'but', 'are', 'mightn', \"they've\", 'aren', \"he's\", 'you', 'yourself', \"haven't\", 'in', \"she'd\", 'shouldn', 'by', 'some', 'who', 'yours', 'with', 'm', \"that'll\", 'it', 'now', 'to', 'very', \"we're\", 'and', 'i', 'his', 'him', \"you'll\", 'few', 'my', 'himself', 'those', \"shouldn't\", 'couldn', 'should', 'just', 'she', \"he'd\", 've', 'a', 'is', \"we'll\", 'll', 'not', 'wasn', 'has', 're', 'hasn', 'once', \"they're\", 'from', \"i'm\", 'don', 'ma', 'do', 'being', 'any', 'too', \"doesn't\", 'all', 'each', 'that', 'here', 'or', 'its', 'this', 'where', 'before', 'shan', 'does', 'most', \"needn't\", 'wouldn', \"couldn't\", \"it'd\", 'up', 'hadn', 'myself', \"didn't\", 'be', 'me', 'mustn', 'having', 'her', 'of', \"should've\", 'were', \"they'll\", 'needn', 'about', 'down', \"hadn't\", 'out', 's', 'off', 'can', 'these', 'because', \"don't\", 'he', 'through', 't', 'them', 'have', 'until', 'above', 'we', 'am', 'as'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)  # Prints the list of stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee876a78-c04d-480d-b5c9-c0da96a1977b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 198\n",
      "Sample stopwords: ['no', 'will', 'same', \"they'd\", 'between', 'into', 'only', 'again', 'further', 'had', 'theirs', 'doing', 'other', 'than', 'their', \"hasn't\", \"won't\", 'your', 'such', \"weren't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(\"Number of stopwords:\", len(stop_words))\n",
    "print(\"Sample stopwords:\", list(stop_words)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d03fe6e-7c03-499e-aa41-aa1de25733c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['this', 'is', 'a', 'sample', 'sentence', 'showing', 'how', 'to', 'remove', 'stopwords', 'and', 'stem', 'words.']\n",
      "Processed tokens: ['sampl', 'sentenc', 'show', 'remov', 'stopword', 'stem', 'words.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Download NLTK stopwords quietly (won't print messages)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample sentence showing how to remove stopwords and stem words.\"\n",
    "\n",
    "tokens = text.lower().split()\n",
    "processed_tokens = [ps.stem(w) for w in tokens if w not in stop_words]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Processed tokens:\", processed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c63bc99-4724-4b2a-accb-dea763ff5fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_id, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_documents(path=\"data/\"):\n",
    "    docs = []\n",
    "    ids = []\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, filename), \"r\", encoding=\"utf8\") as f:\n",
    "                docs.append(f.read())\n",
    "                ids.append(filename)\n",
    "\n",
    "    df = pd.DataFrame({\"doc_id\": ids, \"text\": docs})\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_documents()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82464277-5baa-4552-970d-f8b1a0775737",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Information retrieval retrieves relevant documents.\",\n",
    "    \"TF-IDF is a weighting scheme used in IR.\",\n",
    "    \"The vector space model is commonly applied.\",\n",
    "    \"Python is used in machine learning and IR.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09117212-d0e7-4245-9094-ada46637f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a281da0-a7dd-4203-be56-240758addc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43166456-f6df-443c-a0ee-689280bf2782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         0.064266   0.36226405]\n",
      " [0.         0.064266   1.         0.064266  ]\n",
      " [0.         0.36226405 0.064266   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9d47829-fc76-415d-ab4a-d1f69b2ffb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    index = defaultdict(set)\n",
    "    for i, doc in enumerate(docs):\n",
    "        for token in doc.split():\n",
    "            index[token].add(i)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ffaa99e-1cc2-4668-a08e-6ca50f642c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         0.064266   0.36226405]\n",
      " [0.         0.064266   1.         0.064266  ]\n",
      " [0.         0.36226405 0.064266   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Create documents\n",
    "documents = [\n",
    "    \"Information retrieval retrieves relevant documents.\",\n",
    "    \"TF-IDF is a weighting scheme used in IR.\",\n",
    "    \"The vector space model is commonly applied.\",\n",
    "    \"Python is used in machine learning and IR.\"\n",
    "]\n",
    "\n",
    "# Step 2: TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 3: Cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "436f731d-c447-4ffa-9e77-7cad23649c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_id, text, clean_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove punctuation/numbers\n",
    "    tokens = text.split()  # tokenize\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # remove stopwords\n",
    "    tokens = [ps.stem(t) for t in tokens]  # stemming\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply to all documents\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d11699-ebe2-4a79-9f89-d1d2099487bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tfidf(query, vectorizer, matrix, df, top_k=5):\n",
    "    q_clean = preprocess(query)\n",
    "    q_vec = vectorizer.transform([q_clean])  # vectorizer is already fitted\n",
    "    scores = cosine_similarity(q_vec, matrix)[0]\n",
    "    top_idx = scores.argsort()[::-1][:top_k]\n",
    "    return df.iloc[top_idx][[\"doc_id\", \"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9949691f-426b-4f95-ae8a-653fe26c0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    words = [w for w in text.split() if w not in stop_words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fb4fc34-8b0d-4fd7-ba25-1abf9bf84714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].apply(lambda x: preprocess(x).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "782384b1-2c2c-49df-91bc-2920d320fe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 4)\n",
      "Series([], Name: tokens, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)          # Should show number of rows\n",
    "print(df['tokens'].head())  # Should show list of tokens per document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "817bc46e-f49c-4b08-929b-87cc25ae4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all entries to strings first\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Remove rows where text is empty after stripping\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# Preprocess and tokenize\n",
    "df['tokens'] = df['text'].apply(lambda x: preprocess(x).split())\n",
    "\n",
    "# Keep only rows with non-empty token lists\n",
    "df = df[df['tokens'].map(len) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3720d6b5-f4ff-451f-9239-ccd5c5880f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where tokens list is empty\n",
    "df = df[df['text'].str.strip() != '']  # remove empty text\n",
    "df['tokens'] = df['text'].apply(lambda x: preprocess(x).split())\n",
    "df = df[df['tokens'].map(len) > 0]  # keep only rows with non-empty token lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c77048-1b59-4790-a995-9b67f877f741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw files found: 0\n",
      "Documents after cleaning/tokenization: 0\n",
      "No valid documents found after preprocessing.\n",
      "\n",
      "Building demo corpus because no valid documents were found.\n",
      "Demo corpus created with 3 documents.\n",
      "BM25 index built. Corpus size: 3\n",
      "\n",
      "Top results for query: machine learning algorithms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demo_3.txt</td>\n",
       "      <td>Deep learning is a subset of machine learning ...</td>\n",
       "      <td>0.743683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demo_1.txt</td>\n",
       "      <td>Machine learning is a field of artificial inte...</td>\n",
       "      <td>0.210340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo_2.txt</td>\n",
       "      <td>Information retrieval is the process of obtain...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                               text     score\n",
       "2  demo_3.txt  Deep learning is a subset of machine learning ...  0.743683\n",
       "0  demo_1.txt  Machine learning is a field of artificial inte...  0.210340\n",
       "1  demo_2.txt  Information retrieval is the process of obtain...  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Robust BM25 notebook cell (copy-paste and run)\n",
    "# --------------------------------------------\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Ensure stopwords are available\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    # keep letters and spaces only\n",
    "    text = re.sub(r'[^a-z\\s]+', ' ', text)\n",
    "    tokens = [w for w in text.split() if w and (w not in stop_words)]\n",
    "    return ' '.join(tokens), tokens\n",
    "\n",
    "# Document loader - reads .txt files from \"data/\" folder\n",
    "def load_documents(path=\"data/\"):\n",
    "    docs = []\n",
    "    names = []\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: path '{path}' does not exist.\")\n",
    "        return pd.DataFrame({\"doc_id\": [], \"text\": []})\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        if filename.lower().endswith(\".txt\"):\n",
    "            full = os.path.join(path, filename)\n",
    "            try:\n",
    "                with open(full, \"r\", encoding=\"utf8\") as f:\n",
    "                    txt = f.read()\n",
    "            except Exception as e:\n",
    "                # try with latin-1 as a fallback\n",
    "                try:\n",
    "                    with open(full, \"r\", encoding=\"latin-1\") as f:\n",
    "                        txt = f.read()\n",
    "                except Exception as e2:\n",
    "                    print(f\"Could not read {filename}: {e2}\")\n",
    "                    continue\n",
    "            docs.append(txt)\n",
    "            names.append(filename)\n",
    "    return pd.DataFrame({\"doc_id\": names, \"text\": docs})\n",
    "\n",
    "# 1) Load documents\n",
    "df = load_documents(\"data/\")  # change path if needed\n",
    "\n",
    "# 2) Quick diagnostics before cleaning\n",
    "print(f\"Raw files found: {len(df)}\")\n",
    "\n",
    "# 3) Clean, preprocess and tokenise\n",
    "if len(df) > 0:\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    # remove purely empty or whitespace docs\n",
    "    df = df[df['text'].str.strip() != ''].copy()\n",
    "    # apply preprocess (returns cleaned string and tokens)\n",
    "    cleaned = df['text'].apply(preprocess)\n",
    "    df['clean_text'] = cleaned.apply(lambda x: x[0])\n",
    "    df['tokens'] = cleaned.apply(lambda x: x[1])\n",
    "    # keep only docs with at least 1 token\n",
    "    df = df[df['tokens'].map(len) > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Documents after cleaning/tokenization: {len(df)}\")\n",
    "if len(df) > 0:\n",
    "    # print basic token-length stats and a small sample\n",
    "    print(\"Token length stats: min, median, max =\", df['tokens'].map(len).min(),\n",
    "          int(df['tokens'].map(len).median()), df['tokens'].map(len).max())\n",
    "    print(\"\\nSample doc ids and token counts:\")\n",
    "    print(df[['doc_id']].assign(token_count=df['tokens'].map(len)).head(5).to_string(index=False))\n",
    "else:\n",
    "    print(\"No valid documents found after preprocessing.\")\n",
    "\n",
    "# 4) If corpus is empty, optionally create a tiny demo corpus so you can continue testing\n",
    "if len(df) == 0:\n",
    "    print(\"\\nBuilding demo corpus because no valid documents were found.\")\n",
    "    demo_texts = [\n",
    "        \"Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to learn from data.\",\n",
    "        \"Information retrieval is the process of obtaining information system resources relevant to an information need from a collection of information resources.\",\n",
    "        \"Deep learning is a subset of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.\"\n",
    "    ]\n",
    "    demo_ids = [f\"demo_{i+1}.txt\" for i in range(len(demo_texts))]\n",
    "    df = pd.DataFrame({\"doc_id\": demo_ids, \"text\": demo_texts})\n",
    "    cleaned = df['text'].apply(preprocess)\n",
    "    df['clean_text'] = cleaned.apply(lambda x: x[0])\n",
    "    df['tokens'] = cleaned.apply(lambda x: x[1])\n",
    "    print(\"Demo corpus created with\", len(df), \"documents.\")\n",
    "\n",
    "# 5) Build BM25 index safely\n",
    "corpus_tokens = df['tokens'].tolist()\n",
    "if len(corpus_tokens) == 0:\n",
    "    raise ValueError(\"Corpus token list is empty. Check your data folder and preprocessing steps.\")\n",
    "else:\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "    print(\"BM25 index built. Corpus size:\", len(corpus_tokens))\n",
    "\n",
    "# 6) Retrieval function (returns doc_id, text, score)\n",
    "def retrieve_bm25_with_scores(query, bm25, df, top_k=5):\n",
    "    q_clean, q_tokens = preprocess(query)\n",
    "    if len(q_tokens) == 0:\n",
    "        print(\"Warning: query has no tokens after preprocessing. Try a different query.\")\n",
    "        return pd.DataFrame(columns=[\"doc_id\",\"text\",\"score\"])\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    results = df.iloc[top_idx].copy()\n",
    "    results = results.assign(score = scores[top_idx])\n",
    "    return results[[\"doc_id\",\"text\",\"score\"]]\n",
    "\n",
    "# 7) Test query\n",
    "query = \"machine learning algorithms\"\n",
    "results = retrieve_bm25_with_scores(query, bm25, df, top_k=5)\n",
    "print(\"\\nTop results for query:\", query)\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "047ed8ce-0b49-4860-b00e-ec5f1a51fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all entries to strings first\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Remove rows where text is empty after stripping\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# Preprocess and tokenize\n",
    "df['tokens'] = df['text'].apply(lambda x: preprocess(x).split())\n",
    "\n",
    "# Keep only rows with non-empty token lists\n",
    "df = df[df['tokens'].map(len) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf99fd5-ec36-4527-81c9-a38cbbfc60f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- file1.txt.txt ---\n",
      "Length: 0\n",
      "Preview: ''\n",
      "\n",
      "--- file2.txt.txt ---\n",
      "Length: 0\n",
      "Preview: ''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "docs_path = r\"C:\\projects\\ir-system\\data\"\n",
    "\n",
    "for file in os.listdir(docs_path):\n",
    "    if file.endswith(\".txt\") or file.endswith(\".txt.txt\"):\n",
    "        full_path = os.path.join(docs_path, file)\n",
    "        print(f\"\\n--- {file} ---\")\n",
    "        with open(full_path, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read()\n",
    "            print(\"Length:\", len(content))\n",
    "            print(\"Preview:\", repr(content[:200]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719881ac-731a-4f5a-ab3d-6ab80d9450e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: 2\n",
      "          doc_id text\n",
      "0  file1.txt.txt     \n",
      "1  file2.txt.txt     \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# FIX YOUR PATH HERE\n",
    "docs_path = r\"C:\\projects\\ir-system\\data\"\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Load .txt files\n",
    "for filename in os.listdir(docs_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(docs_path, filename), \"r\", encoding=\"utf8\") as f:\n",
    "            text = f.read().strip()\n",
    "            documents.append({\"doc_id\": filename, \"text\": text})\n",
    "\n",
    "df = pd.DataFrame(documents)\n",
    "\n",
    "print(\"Loaded documents:\", len(df))\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9f3369-fe89-4b8c-a9dd-983b9aa7a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty documents: 0\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"text\"].str.strip() != \"\"]\n",
    "print(\"Non-empty documents:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6cc0422-5d5a-4729-8b62-3910dc71d339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents = 0\n",
      "Empty DataFrame\n",
      "Columns: [doc_id, text]\n",
      "Index: []\n",
      "\n",
      "Tokenized docs:\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded documents =\", len(df))\n",
    "print(df[[\"doc_id\", \"text\"]].head())\n",
    "\n",
    "print(\"\\nTokenized docs:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(row[\"doc_id\"], \"â†’\", row[\"tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f271cedb-81df-4684-9c05-7664a4e9c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILES FOUND: ['file1.txt.txt', 'file2.txt.txt']\n",
      "\n",
      "--- DOCUMENT CONTENT CHECK ---\n",
      "\n",
      "FILE: file1.txt.txt\n",
      "Length of file: 0\n",
      "Preview: ''\n",
      "\n",
      "FILE: file2.txt.txt\n",
      "Length of file: 0\n",
      "Preview: ''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "docs_path = r\"C:\\projects\\ir-system\\data\"\n",
    "\n",
    "print(\"FILES FOUND:\", os.listdir(docs_path))\n",
    "\n",
    "print(\"\\n--- DOCUMENT CONTENT CHECK ---\")\n",
    "for filename in os.listdir(docs_path):\n",
    "    if filename.endswith(\".txt\") or filename.endswith(\".txt.txt\"):\n",
    "        full = os.path.join(docs_path, filename)\n",
    "        with open(full, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read()\n",
    "            print(f\"\\nFILE: {filename}\")\n",
    "            print(\"Length of file:\", len(content))\n",
    "            print(\"Preview:\", repr(content[:200]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7c704be-1f3a-4714-87b9-124442e4624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  doc_id                                               text\n",
      "0   doc1  Machine learning is a field of AI that uses da...\n",
      "1   doc2  Information retrieval systems help users find ...\n",
      "2   doc3  Deep learning is a subset of machine learning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SYED ZAIN UL\n",
      "[nltk_data]     AIDEEN/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# ------------------------------\n",
    "# DEMO documents\n",
    "# ------------------------------\n",
    "documents = [\n",
    "    {\"doc_id\": \"doc1\", \"text\": \"Machine learning is a field of AI that uses data to train models.\"},\n",
    "    {\"doc_id\": \"doc2\", \"text\": \"Information retrieval systems help users find relevant documents efficiently.\"},\n",
    "    {\"doc_id\": \"doc3\", \"text\": \"Deep learning is a subset of machine learning using neural networks.\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(documents)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe1f98c-c215-464a-bb3e-a472caddecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  doc_id                                               text  \\\n",
      "0   doc1  Machine learning is a field of AI that uses da...   \n",
      "1   doc2  Information retrieval systems help users find ...   \n",
      "2   doc3  Deep learning is a subset of machine learning ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [machine, learning, is, a, field, of, ai, that...  \n",
      "1  [information, retrieval, systems, help, users,...  \n",
      "2  [deep, learning, is, a, subset, of, machine, l...  \n"
     ]
    }
   ],
   "source": [
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x.lower()))\n",
    "df = df[df[\"tokens\"].map(len) > 0]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf9bfa56-89f0-463d-a951-fc85882eb83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index built successfully!\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = df[\"tokens\"].tolist()\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "print(\"BM25 index built successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f453a84a-6f6b-4661-b4e5-56489d6d0f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc3</td>\n",
       "      <td>Deep learning is a subset of machine learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1</td>\n",
       "      <td>Machine learning is a field of AI that uses da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc2</td>\n",
       "      <td>Information retrieval systems help users find ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                               text\n",
       "2   doc3  Deep learning is a subset of machine learning ...\n",
       "0   doc1  Machine learning is a field of AI that uses da...\n",
       "1   doc2  Information retrieval systems help users find ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_bm25(query, bm25, df, top_k=5):\n",
    "    q_tokens = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    top_idx = scores.argsort()[::-1][:top_k]\n",
    "    return df.iloc[top_idx][[\"doc_id\", \"text\"]]\n",
    "\n",
    "query = \"machine learning algorithms\"\n",
    "top_docs = retrieve_bm25(query, bm25, df)\n",
    "top_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e02aad-1993-4e52-adac-420522a718d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bm25(query, bm25, df, top_k=5):\n",
    "    q_clean = preprocess(query)\n",
    "    q_tokens = q_clean.split()\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    return df.iloc[top_idx][[\"doc_id\", \"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "672b7fdc-f0ba-4183-81c8-58a3a5ee5004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid .txt files found. Using demo documents...\n",
      "Loaded 3 documents\n",
      "Docs with tokens: 3\n",
      "BM25 index built successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SYED ZAIN UL\n",
      "[nltk_data]     AIDEEN/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc3</td>\n",
       "      <td>Deep learning is a subset of machine learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1</td>\n",
       "      <td>Machine learning is a field of AI that uses da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc2</td>\n",
       "      <td>Information retrieval systems help users find ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                               text\n",
       "2   doc3  Deep learning is a subset of machine learning ...\n",
       "0   doc1  Machine learning is a field of AI that uses da...\n",
       "1   doc2  Information retrieval systems help users find ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 0: Imports\n",
    "# -----------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "nltk.download(\"punkt\")  # only downloads if missing\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load documents\n",
    "# -----------------------------\n",
    "docs_path = r\"C:\\projects\\ir-system\\data\"  # change if your folder is elsewhere\n",
    "\n",
    "documents = []\n",
    "\n",
    "if os.path.exists(docs_path):\n",
    "    files = os.listdir(docs_path)\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(docs_path, filename), \"r\", encoding=\"utf8\") as f:\n",
    "                text = f.read().strip()\n",
    "                if text:\n",
    "                    documents.append({\"doc_id\": filename, \"text\": text})\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Fallback demo corpus\n",
    "# -----------------------------\n",
    "if len(documents) == 0:\n",
    "    print(\"No valid .txt files found. Using demo documents...\")\n",
    "    documents = [\n",
    "        {\"doc_id\": \"doc1\", \"text\": \"Machine learning is a field of AI that uses data to train models.\"},\n",
    "        {\"doc_id\": \"doc2\", \"text\": \"Information retrieval systems help users find relevant documents efficiently.\"},\n",
    "        {\"doc_id\": \"doc3\", \"text\": \"Deep learning is a subset of machine learning using neural networks.\"}\n",
    "    ]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(documents)\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Tokenize\n",
    "# -----------------------------\n",
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x.lower()))\n",
    "df = df[df[\"tokens\"].map(len) > 0]\n",
    "print(f\"Docs with tokens: {len(df)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Build BM25 index\n",
    "# -----------------------------\n",
    "tokenized_docs = df[\"tokens\"].tolist()\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "print(\"BM25 index built successfully!\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Define retrieval function\n",
    "# -----------------------------\n",
    "def retrieve_bm25(query, bm25, df, top_k=5):\n",
    "    q_tokens = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    top_idx = scores.argsort()[::-1][:top_k]\n",
    "    return df.iloc[top_idx][[\"doc_id\", \"text\"]]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Run example query\n",
    "# -----------------------------\n",
    "query = \"machine learning algorithms\"\n",
    "top_docs = retrieve_bm25(query, bm25, df)\n",
    "top_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf787933-ff9c-477a-83dd-d6ca1d0f9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(df['tokens'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "628cede1-7417-437e-8c07-86f740ca2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_docs, relevant_docs, k=5):\n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    return len(set(retrieved_k) & set(relevant_docs)) / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d0b63-b9de-468c-9b78-ab8a8216973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter your search query: \")\n",
    "print(\"Top documents (TF-IDF):\")\n",
    "display(retrieve_tfidf(query, vectorizer, tfidf_matrix, df))\n",
    "\n",
    "print(\"Top documents (BM25):\")\n",
    "display(retrieve_bm25(query, bm25, df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fe91c-1819-4119-8cc7-2395f758284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Information Retrieval System\n",
    "\n",
    "## Installation\n",
    "pip install -r requirements.txt\n",
    "\n",
    "## Run\n",
    "1. Open Jupyter Notebook\n",
    "2. Run all cells\n",
    "3. Use the search demo cell to query documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a34b7-89c7-4e08-898c-3c7290560112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
